<!DOCTYPE html>
<html>

<head>
    <title>A sample classification with Sklearn</title>
    <meta name="description" content="Implement a A sample classification with Sklearn">
    <meta name="keywords" content="AI, Machine Learning, Python, Sklearn">

    <meta http-equiv="x-ua-compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="author" content="Vikas K. Solegaonkar">
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="vendor/highlight/styles/default.css">
    <link href="css/blog-post.css" rel="stylesheet">
</head>

<body>
    <div id="body">

        <h1>A sample classification with Sklearn</h1>
        <hr />
        <p>ScikitLearn continues to be one of the popular machine learning libraries in Python. It provides ready modules for implementing most machine learning algorithms. These are very simple to
            use, yet they are extremely configurable. </p>
        <p>Just as all machine learning code, SktLearn is concept heavy and code lite. The code is trivially small. Just a couple of lines will do the job. But it is very important that we understand
            the core concepts before we can use it meaningfully. To get a feel of how it works, let us go through a classification problem.</p>
        <h2>The Iris Dataset</h2>
        <hr />
        <p>This is a small data set of just 150 records. That may not be enough for solving a real life problem. But, because of the small size, it is often quoted in the academic world. It refers to
            classification of a flower called Iris. There are three major species of this plant - Iris Setosa, Iris Versicolour, Iris Virginica. The Iris data set measures the petal and sepals of the
            flowers - to provide data for a classification.</p>
        <p>For 150 samples of flowers of these species, it provides the following data</p>
        <ul>
            <li>Sepal length in cm </li>
            <li>Sepal width in cm </li>
            <li>Petal length in cm </li>
            <li>Petal width in cm </li>
            <li>Species</li>
        </ul>
        <div class='row'><img class="img-responsive col-sm-4" src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/220px-Iris_versicolor_3.jpg' /><img
                class="img-responsive col-sm-4"
                src='https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg/220px-Kosaciec_szczecinkowaty_Iris_setosa.jpg' /><img
                class="img-responsive col-sm-4" src='https://upload.wikimedia.org/wikipedia/commons/thumb/9/9f/Iris_virginica.jpg/220px-Iris_virginica.jpg' /></div>
        <p>Just search on Google for the Iris Data Set and we can find many copies of this data referred all over the machine learning literature. We can download the data set in a CSV file on our
            disk - so that we can play around with it.</p>
        <h2>Analyze the Data</h2>
        <hr />
        <p>Before jumping into the implementation of the machine learning model, it is always a good practice to peep into the data (at least a small sample), to understand how it looks. This can
            provide a great help in deciding which algorithm could be more suitable. So, let us work on that.</p>
        <p>Let us start with importing the necessary Python modules</p>
        <pre><code class='python'>import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt</code></pre>
        <p>We need to configure the Seaborn and MatplotLib - so that we can see the output on the JupyterNotebook that we use for the work. We can also set the Seaborn pallet to get the colors of our
            choice</p>
        <pre><code class='python'>sns.set_palette('husl')
%matplotlib inline</code></pre>
        <p>With this in place, let us load the Iris data into a Pandas Data Frame. We need to give the appropriate path to the CSV file so that it can be loaded correctly.</p>
        <pre><code class='python'>data = pd.read_csv('Iris.csv')</code></pre>
        <p>The next step is to look at the data. 150 records may be a manageable count. But in a typical problem, we can expect a lot more than that. We cannot read through all the records. But
            peeping into the first few records is quite helpful.</p>
        <pre><code class='python'>data.head()</code></pre>
        <table>
            <tr>
                <th>&nbsp; </th>
                <th>Id </th>
                <th>SepalLengthCm </th>
                <th>SepalWidthCm </th>
                <th>PetalLengthCm </th>
                <th>PetalWidthCm </th>
                <th>Species</th>
            </tr>
            <tr>
                <td>0 </td>
                <td>1 </td>
                <td>5.1 </td>
                <td>3.5 </td>
                <td>1.4 </td>
                <td>0.2 </td>
                <td>Iris-setosa</td>
            </tr>
            <tr>
                <td>1 </td>
                <td>2 </td>
                <td>4.9 </td>
                <td>3.0 </td>
                <td>1.4 </td>
                <td>0.2 </td>
                <td>Iris-setosa</td>
            </tr>
            <tr>
                <td>2 </td>
                <td>3 </td>
                <td>4.7 </td>
                <td>3.2 </td>
                <td>1.3 </td>
                <td>0.2 </td>
                <td>Iris-setosa</td>
            </tr>
            <tr>
                <td>3 </td>
                <td>4 </td>
                <td>4.6 </td>
                <td>3.1 </td>
                <td>1.5 </td>
                <td>0.2 </td>
                <td>Iris-setosa</td>
            </tr>
            <tr>
                <td>4 </td>
                <td>5 </td>
                <td>5.0 </td>
                <td>3.6 </td>
                <td>1.4 </td>
                <td>0.2 </td>
                <td>Iris-setosa</td>
            </tr>
        </table>
        <p>Note that the column Id comes from the file and the Sr number is from the Pandas Data Frame.</p>
        <p>Next, we can check out the data in a wider scope. What are the data types?, Do we have any missing values?, etc. With this information, we can take some implementation level decisions.</p>
        <pre><code class='python'>data.info()</code></pre>
        <p>This generates output:</p>
        <pre><code class='python'>&gt;class 'pandas.core.frame.DataFrame'&lt;
RangeIndex: 150 entries, 0 to 149
Data columns (total 6 columns):
Id               150 non-null int64
SepalLengthCm    150 non-null float64
SepalWidthCm     150 non-null float64
PetalLengthCm    150 non-null float64
PetalWidthCm     150 non-null float64
Species          150 non-null object
dtypes: float64(4), int64(1), object(1)
memory usage: 7.1+ KB</code></pre>
        <p>Next, we can look up some statistical information about the available data. What is the kind of values we have? What is the mean, variance, min / max values, percentiles, quartiles... These
            give us some more ideas about how the model should look.</p>
        <pre><code class='python'>data.describe()</code></pre>
        <p>This gives us a table</p>
        <table>
            <tr>
                <th>&nbsp; </th>
                <th>Id </th>
                <th>SepalLengthCm </th>
                <th>SepalWidthCm </th>
                <th>PetalLengthCm </th>
                <th>PetalWidthCm</th>
            </tr>
            <tr>
                <td>count </td>
                <td>150.000000 </td>
                <td>150.000000 </td>
                <td>150.000000 </td>
                <td>150.000000 </td>
                <td>150.000000</td>
            </tr>
            <tr>
                <td>mean </td>
                <td>75.500000 </td>
                <td>5.843333 </td>
                <td>3.054000 </td>
                <td>3.758667 </td>
                <td>1.198667</td>
            </tr>
            <tr>
                <td>std </td>
                <td>43.445368 </td>
                <td>0.828066 </td>
                <td>0.433594 </td>
                <td>1.764420 </td>
                <td>0.763161</td>
            </tr>
            <tr>
                <td>min </td>
                <td>1.000000 </td>
                <td>4.300000 </td>
                <td>2.000000 </td>
                <td>1.000000 </td>
                <td>0.100000</td>
            </tr>
            <tr>
                <td>25% </td>
                <td>38.250000 </td>
                <td>5.100000 </td>
                <td>2.800000 </td>
                <td>1.600000 </td>
                <td>0.300000</td>
            </tr>
            <tr>
                <td>50% </td>
                <td>75.500000 </td>
                <td>5.800000 </td>
                <td>3.000000 </td>
                <td>4.350000 </td>
                <td>1.300000</td>
            </tr>
            <tr>
                <td>75% </td>
                <td>112.750000 </td>
                <td>6.400000 </td>
                <td>3.300000 </td>
                <td>5.100000 </td>
                <td>1.800000</td>
            </tr>
            <tr>
                <td>max </td>
                <td>150.000000 </td>
                <td>7.900000 </td>
                <td>4.400000 </td>
                <td>6.900000 </td>
                <td>2.500000</td>
            </tr>
        </table>
        <p>We can also check out the number of samples of each type. Do we have enough representation of each type? If each type is not sufficiently represented in the data set, the model will be
            naturally biased against that type.</p>
        <pre><code class='python'>data['Species'].value_counts()</code></pre>
        <p>We get the output</p>
        <pre><code class='python'>Iris-virginica     50
Iris-versicolor    50
Iris-setosa        50
Name: Species, dtype: int64</code></pre>
        <p>Note that this is an academic data set and so everything is just what it should be! In a real scenario, we will never get things laid out so well.</p>
        <h2>Plotting the Data</h2>
        <hr />
        <p>Numbers don't speak enough. To get a better feel of the data, and dig deeper, we can use plots. These are visual representations of the available data. These help us develop a mental
            picture of the data. In a real case study, we would work on a small sample set of the available data. But in this case, we can use the entire data set.</p>
        <h3>Pair Plot</h3>
        <hr />
        <p>The pair plots are a helpful way to identify how the different input features are related to each other, and to the final outcome. Here, we get a graphical representation of each
            combination of two input features</p>
        <pre><code class='python'>tmp = data.drop('Id', axis=1)
g = sns.pairplot(tmp, hue='Species', markers='+')
plt.show()</code></pre>
        <div class='center'><img src='img/iris_pair_plot.png' /></div>
        <p>Note that some combinations give a clear separation of the data into different classes, while others are mixed up. This gives us a hint about which features are enough to do the job. In
            case of a performance issue or an extreme overfitting, we might have to drop certain features to make things work better. Pair plots can easily point out the redundant ones.</p>
        <h3>Violin Plots</h3>
        <hr />
        <p>Another useful representation is the violin plot. This helps us identify how the data is laid out with respect to a given input feature. For example, if we plot the data by the sepal
            length:</p>
        <pre><code class='python'>g = sns.violinplot(y='Species', x='SepalLengthCm', data=data, inner='quartile')
plt.show()</code></pre>
        <div class='center'><img src='img/iris_violin_sepal_length.png' /></div>
        <p>The dotted lines give us an idea about the quartiles (25%, 50% and 75%).</p>
        <p>Similarly, we can check up the violin plots for the other features:</p>
        <pre><code class='python'>g = sns.violinplot(y='Species', x='SepalWidthCm', data=data, inner='quartile')
plt.show()</code></pre>
        <div class='center'><img src='img/iris_violin_sepal_width.png' /></div>
        <pre><code class='python'>g = sns.violinplot(y='Species', x='PetalLengthCm', data=data, inner='quartile')
plt.show()</code></pre>
        <div class='center'><img src='img/iris_violin_petal_length.png' /></div>
        <pre><code class='python'>g = sns.violinplot(y='Species', x='PetalWidthCm', data=data, inner='quartile')
plt.show()</code></pre>
        <div class='center'><img src='img/iris_violin_petal_width.png' /></div>
        <h2>Classification</h2>
        <hr />
        <p>Having developed an idea about the data available, we can now jump into building the classifier. Based on the analysis of the data and the plots we saw above, we can note that Iris-setosa
            is reasonably separated from the others. The others are quite close to each other until we consider the petal as well as the sepal data. The prior suggests a good case for decision trees
            and the latter suggests regression. </p>
        <p>In this case, we can check up many different algorithms and check out how they look. SktLearn allows us to configure hyperparameters for each of these algorithms - which can greatly impact
            the performance. But for now, we will just use the defaults and see how this works.</p>
        <p>First, we need to rearrange and split the data to get things setup. Let us start with importing the necessary modules</p>
        <pre><code class='python'>from sklearn import metrics
from sklearn.model_selection import train_test_split</code></pre>
        <p>Now we should separate the input and output values. The sepal/petal width/length are the input features. The class is the output.</p>
        <pre><code class='python'>X = data.drop(['Id', 'Species'], axis=1)
y = data['Species']</code></pre>
        <p>Next, we need to split the data into the train and test sets.</p>
        <pre><code class='python'>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=5)</code></pre>
        <p>Let us now start with individual algorithms</p>
        <ul>
            <li>Logistic Regression</li>
        </ul>
        <pre><code class='python'>from sklearn.linear_model import LogisticRegression
clf = LogisticRegression()
clf.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(clf.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(clf.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.944 and test set accuracy as 0.933</p>
        <ul>
            <li>Nearest Neighbor</li>
        </ul>
        <pre><code class='python'>from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier()
clf.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(clf.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(clf.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.967 and test set accuracy as 0.967</p>
        <ul>
            <li>Decision Tree</li>
        </ul>
        <pre><code class='python'>from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(clf.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(clf.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 1 and test set accuracy as 0.95</p>
        <ul>
            <li>Random Forest</li>
        </ul>
        <pre><code class='python'>from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
clf.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(clf.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(clf.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.978 and test set accuracy as 0.967. Note that the Decision Tree model was badly overfitting the data. But the Random Forest gave much better
            results.</p>
        <ul>
            <li>Support Vector Machine</li>
        </ul>
        <pre><code class='python'>from sklearn import svm
clf = svm.SVC()
clf.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(clf.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(clf.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.978 and test set accuracy as 0.983</p>
        <ul>
            <li>Stochastic Gradient Descent</li>
        </ul>
        <pre><code class='python'>from sklearn.linear_model import SGDClassifier
clf = SGDClassifier()
clf.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(clf.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(clf.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.689 and test set accuracy as 0.683</p>
        <p>This is pathetic! Also, if we run this multiple times, we can notice a huge variation in the results. We can expect this because the data is too small. SGD is very good for high volume
            flowing data. We do not need all the data at once. But we do need a good amount of data.</p>
        <ul>
            <li>Gaussian Processes</li>
        </ul>
        <pre><code class='python'>from sklearn.gaussian_process import GaussianProcessClassifier
clf = GaussianProcessClassifier()
clf.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(clf.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(clf.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.944 and test set accuracy as 0.967</p>
        <ul>
            <li>Multi Layer Perceptrons</li>
        </ul>
        <pre><code class='python'>from sklearn.neural_network import MLPClassifier
clf = MLPClassifier()
clf.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(clf.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(clf.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.989 and test set accuracy as 0.950</p>
        <p>Please note that these are sample evaluations on a very small data set. So not every observation would hold on in a real life scenario.</p>

    </div>
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="vendor/highlight/highlight.pack.js"></script>
    <script src="scripts/blog.js"></script>
</body>

</html>