<!DOCTYPE html>
<html>

<head>
    <title>A KNN classifier using Sklearn</title>
    <meta name="description" content="Implement a A KNN classifier with Sklearn">
    <meta name="keywords" content="AI, Machine Learning, Python, Sklearn, knn">

    <meta http-equiv="x-ua-compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="author" content="Vikas K. Solegaonkar">
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="vendor/highlight/styles/default.css">
    <link href="css/blog-post.css" rel="stylesheet">
</head>

<body>
    <div id="body">

        <h1>A KNN classifier using Sklearn</h1>
        <hr />

        <p>Classification is an important subject in Supervised Learning. A major part of machine learning applications deal with binary outputs which require classification rather than regression.
            KNN Classifier is one of the many classification algorithms.</p>
        <p>Like most Machine Learning algorithms, the KNN algorithm is also inspired by a tendency of the human mind - to go along with the crowd. Conceptually, KNN just looks at the known points
            around the query point and predicts that its outcome is similar to the points around it. More precisely, For any new point, it checks for the K points that are closest in terms of the
            defined distance metric. Once these are identified, the outcome of each of those points is identified based on the training set. And the outcome of the new point is defined based on the
            highest bidder in the neighborhood. For example, if we look for the 5 nearest of a given test point, if 3 of those points say positive and two say negative, the outcome is predicted as
            positive since that is the highest bidder.</p>
        <h2>Implementation</h2>
        <hr />
        <p>In Python code. we can use SciKitLearn to do this very easily. The foremost step is to Import the necessary packages</p>
        <pre><code class='python'>from sklearn.datasets import load_breast_cancer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split</code></pre>
        <p>The SciKtLearn package comes with several data sets built into it. We can use the breast cancer dataset to start with this example</p>
        <pre><code class='python'>cancer = load_breast_cancer()</code></pre>
        <h3>Breast Cancer Dataset</h3>
        <hr />
        <p>You can get a glimpse of what goes into this dataset</p>
        <pre><code class='python'>cancer.data.shape
(569, 30)</code></pre>
        <p>It has 569 records with 30 features per record. That is too small to get a useful model, but good enough for learning. These are the features for each record in the data set.</p>
        <pre><code class='python'>print(cancer.feature_names)</code></pre>
        <pre><code class='python'>['mean radius' 'mean texture' 'mean perimeter' 'mean area'
    'mean smoothness' 'mean compactness' 'mean concavity'
    'mean concave points' 'mean symmetry' 'mean fractal dimension'
    'radius error' 'texture error' 'perimeter error' 'area error'
    'smoothness error' 'compactness error' 'concavity error'
    'concave points error' 'symmetry error' 'fractal dimension error'
    'worst radius' 'worst texture' 'worst perimeter' 'worst area'
    'worst smoothness' 'worst compactness' 'worst concavity'
    'worst concave points' 'worst symmetry' 'worst fractal dimension']</code></pre>
        <p>It has only two target values for classification. Either it is malignant of benign</p>
        <pre><code class='python'>print(cancer.target_names)</code></pre>
        <pre><code class='python'>['malignant' 'benign']</code></pre>
        <h3>KNN Classifier</h3>
        <hr />
        <p>Now we can use the KNN Classifier provided by SciKitLearn to process the data. You can start with splitting the data into the training and test sets.</p>
        <pre><code class='python'>X_train, X_test, Y_train, Y_test = train_test_split(cancer.data,cancer.target, random_state=42)</code></pre>
        <p>Next, instantiate an object of the KNN Classifier. For now, just let it pick up the default values.</p>
        <pre><code class='python'>knn = KNeighborsClassifier()</code></pre>
        <p>Next, you can use the fit() method to update the model in order to fit the training set.</p>
        <pre><code class='python'>knn.fit(X_train, Y_train)</code></pre>
        <pre><code class='python'>KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
            metric_params=None, n_jobs=1, n_neighbors=5, p=2,
            weights='uniform')</code></pre>
        <h3>Evaluate the Model</h3>
        <hr />
        <p>You can now evaluate the model using the scores for the training and the test sets.</p>
        <pre><code class='python'>>>> knn.score(X_train, Y_train)
0.93427230046948362

>>> knn.score(X_test, Y_test)
0.965034965034965</code></pre>
        <p>Note that the score for the training set is quite similar to the score for the test set. This means that we do not have a problem of over fitting. The score of 0.93 may or may not be good -
            based on the requirements of the application. We can try to improve on it by tuning some of the hyperparameters.</p>
        <h3>Conclusion</h3>
        <hr />
        <p>The KNN Classifier is a good tool for classification when the size of the data and the features are within control - else the computation can get very expensive. The classifier accuracy is
            based on the assumption that the similar points are geometrically close to each other - which may not always be the case. Consider for example, data sets in form of two concentric circles
            - the inner circle being positive and the outer circle negative. In such a case, inventing new distance metric may help to an extent. But the cost of computation increases very rapidly
            with the complexity of the distance metric. The cost also increases rapidly with the number of features.</p>
        <p>But it is a very elegant and intuitive way of classifying when the data is good.</p>

    </div>
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="vendor/highlight/highlight.pack.js"></script>
    <script src="scripts/blog.js"></script>
</body>

</html>