<!DOCTYPE html><html><head><title>Linear Algebra - a Refresher</title><meta name="description" content="Refresher of Linear Algebra basics for Machine Learning"><meta name="keywords" content="AI,Machine Learning,Linear Algebra"><script src="../scripts/index.js"></script></head><body><h1>Linear Algebra - a Refresher</h1><hr/>

<p>Mathematics is the basis of any Engineering. Much more so with abstract sciences like Machine Learning. And Linear Algebra is at the core. It is very important to understand linear algebra if we want to proceed with understanding Machine Learning.</p><p>Linear Algebra was developed to simplify linear equations. It provides a simple way of representing and formalizing the solution of linear equations.</p><p>Consider the two equations below</p><pre><code class='python'>2x + y = 10
2x - y = 2</code></pre><p>One can trivially solve these two equations of two variables. That is high school algebra. We don't need any code to solve it. But what would we do if we were given 100,000 equations of 100,000 variables? Linear Algebra helps us here. Let us see how</p><p>Of course, we do not have enough space here to write down the 100,000 equations. But we can use the above two equations to understand the concept. These two equations can be written in matrix form as</p><div class='center'><img src='img/linalg01.png' /></div><p>Essentially, we have represented the set of equations in the form</p><pre><code class='python'>    Ax = b       # Where A is a matrix; x and b are vectors.</code></pre><p>This is the short hand way of representing a set of linear equations - in form of matrices. </p><h2>Notations</h2><hr/><p>Reducing the representational size is not the only advantage of this. We will see below how it helps in computation. Before that, let us look into the notifications.</p><ul><li>A &isin; R<sup>m x n</sup>   =&gt; A is a matrix of m rows and n columns, and all its elements are real numbers.</li><li>x &isin; R<sup>n</sup>  =&gt; x is a vector with n entries and all its elements are real numbers.</li><li>A<sub>ij</sub>  =&gt; Denotes the element of A at the i<sup>th</sup> row and j<sup>th</sup> column</li><li>x<sub>i</sub>  =&gt; Denotes the element of i<sup>th</sup> element of the vector x</li><li>A<sub>i,:</sub>  => i<sup>th</sup> row in the matrix A</li><li>A:,j => j<sup>th</sup> column in the matrix A</li></ul><p>Intuitively, we can think of a vector as a point in n-dimensional space. And a matrix A as an operation that can map a vector V<sub>1</sub> in n dimensional space into another vector V<sub>2</sub> in m dimensional space.</p><h2>Important Concepts</h2><hr/><p>Linear Algebra is a vast domain. In order to use it in Machine Learning, it is necessary to understand some basic concepts.</p><h3>Matrix Multiplication</h3><hr/><p>The concept of matrix multiplication is not so intuitive. Suppose we have two matrices A &isin; R<sup>m x n</sup> and B &isin; R<sup>n x p</sup>, the product of the two matrices is C &isin; R<sup>m x p</sup> where</p><pre><code class='python'>    C<sub>ij</sub> = &sum; A<sub>ik</sub> B<sub>kj</sub></code></pre><p>Note the sizes of of the three matrices. For the product A x B to exist, the number of columns in the matrix A must be the same as the number of rows in matrix B. In this case, the product will have rows as many as A and columns as many as B</p><p>There could be 4 possibilities of matrix product. Matrix - Matrix product, Matrix - Vector product, Vector - Matrix product and Vector - Vector product. Let us look at two special cases here:</p><h3>Vector - Vector Product</h3><hr/><div class='center'><img src='img/linalg02.png' /></div><p>Given two vectors x, y &isin; R<sup>n</sup>, x<sup>T</sup>y - the dot product of the two vectors - is a real number.</p><h3>Matrix-Vector Product</h3><hr/><div class='center'><img src='img/linalg03.png' /></div><p>Given a matrix A &isin; R<sup>m x n</sup> and a vector x &isin; R<sup>n</sup>. Their product y = Ax is a vector &isin; R<sup>m</sup>. In other words, y is a linear combination of the columns of A, where the coefficients of the linear combination are given by the entries of x.</p><p>It is interesting to note here that matrix multiplication</p><ul><li>Is not commutative. A x B may not be equal to B x A. In fact B x A may not even exist.</li><li>But it is distributive. A x (B + C) = A x B + A x B</li><li>And it is also associative. A x (B x C) = (A x B) x C</li></ul><h3>Identity Matrix</h3><hr/><p>By definition, multiplicative identity is a matrix I such that A x I = A. Here, I happens to be a matrix where all the diagonal elements are 1 and others are 0. In more formal terms, for a matrix A &isin; R<sup>m x n</sup>, the identity matrix is the matrix I &isin; R<sup>n x n</sup> such that I<sub>ij</sub> = 1 if i = j and 0 otherwise.</p><p>In general the size of an identity matrix is not important. Just that it should be a square matrix (with equal number of rows and columns) such that I<sub>ij</sub> = 1 if i = j and 0 otherwise.</p></p><h3>Transpose</h3><hr/><p>The transpose of a matrix obtained by flipping it around the diagonal. In formal terms, for a matrix A &isin; R<sup>m x n</sup>, the transpose A<sup>T</sup> is matrix B &isin; R<sup>n x m</sup> such that B<sub>ij</sub> = A<sub>ji</sub></p><p>Interesting properties of transpose operation</p><ul><li>(A<sup>T</sup>)<sup>T</sup> = A</li><li>(A x B)<sup>T</sup> = B<sup>T</sup> x A<sup>T</sup></li><li>(A + B)<sup>T</sup> = A<sup>T</sup> + B<sup>T</sup></li></ul><h3>Symmetric Matrix</h3><hr/><p>A symmetric matrix that is symmetric along the diagonal. In formal terms, matrix A &isin; R<sup>n x n</sup> is symmetric if A<sup>T</sup> = A. And matrix A &isin; R<sup>n x n</sup> is anti-symmetric if A<sup>T</sup> = -A</p><p>Note that the matrix has to be a square matrix in order to be a symmetric or anti-symmetric.</p><p>Observe that for any matrix A, A + A<sup>T</sup>  is symmetric and A - A<sup>T</sup> is anti-symmetric. Thus, any matrix A = ((A + A<sup>T</sup>) + (A - A<sup>T</sup>))/2. That means, any matrix A can be expressed as a sum of a symmetric matrix and an anti-symmetric matrix. </p><p>This property is important for subsequent derivations. Symmetric matrices have very nice properties that are useful in real life calculations. It is common to denote a symmetric matrix using S. Thus, A &isin; S<sup>n</sup> means that A is an n x n dimensional symmetric matrix.</p><h3>The Trace</h3><hr/><p>Trace of a square matrix A &isa; R<sup>n x n</sup> is denoted by tr(A). It is the sum of all the diagonal elements of the matrix.</p><pre><code class='python'>    tr(A) = &sum; A<sub>ii</sub></code></pre><p>The matrix trace has some interesting properties:</p><ul><li>tr(A) = tr(A<sup>T</sup>)</li><li>tr(A + B) = tr(A) + tr(B)</li><li>tr(n x A) = n x tr(A)</li><li>If A x B is a square matrix (implying that B x A is also a square matrix), tr(AB) = tr(BA)</li><li>Extending this, A, B, C, such that ABC is a square matrix, tr(ABC) = tr(BCA) = tr(CAB). </li><li>This is not limited to 2 or 3, it applies to any number of matrices</li></ul><h3>Norm</h3><hr/><p>We can think of the Norm of a vector as the length of the segment from the origin to the point denoted by the vector - that is the Euclidean norm (or the L<sub>s</sub> Norm. It is denoted as</p><pre><code class='python'>    ||x||<sub>2</sub> = sqrt(&sum; x<sub>i</sub><sup>2</sup>)</code></pre><p>Note that ||x||<sub>2</sub> is same as x<sub>T</sub>x - or the dot product of x with itself.</p><p>The L<sub>1</sub> norm is defined as</p><pre><code class='python'>    ||x||<sub>1</sub> = &sum; |x|<sub>i</sub></code></pre><p>Similarly, we can define L<sub>p</sub> norm as</p><pre><code class='python'>    ||x||<sub>p</sub> = (&sum; x<sub>i</sub><sup>p</sup>)<sup>1/p</sup></code></pre><p>If we consider p = infinity, the L<sub> &infin;</sub> simplifies to</p><pre><code class='python'>    L<sub> &infin;</sub> = max(|x<sub>i</sub>|)</code></pre><p>In fact, the Norm is not limited to these L<sub>n</sub> norms. We can define our own function. Any function f(x) can be used as the Norm if and only if</p><ul><li>f(x) &gt;= 0 for all x [non-negativity]</li><li>f(x) = 0 if and only if x = 0 [definiteness]</li><li>For all x &isin; R<sup>n</sup> and t &isin; R, f(tx) = |t|f(x) [homogeneity]</li><li>For all x &isin; R<sup>n</sup> and y &isin; R<sup>n</sup>, f(x + y) &lt;= f(x) + f(y) [triangle inequality]</li></ul><p>Norms are also defined for Matrices. The Frobenius Norm for matrices is defined as</p><pre><code class='python'>    ||A||<sub>F</sub> = (&sum; &sum; A<sub>ij</sub><sup>2</sup>)<sup>1/2</sup></code></pre><p>Interesting to note that this is also equal to tr(A<sup>T</sup>A)<sup>1/2</sup></p><p>Mathematicians have defined many other kinds of Norms. But these are the important ones we will need for machine learning.</p>
<h3>Linear Independence &amp; Rank</h3><hr/><p>A set of n vectors {x<sub>1</sub>, x<sub>2</sub>, . . . x<sub>n</sub>} &isin; R<sup>n</sup> is called linearly independent if no vector can be expressed as a combination of the other vectors. Conversely, a set of n vectors is called linearly dependent if at least one of them can be expressed as</p><pre><code class='python'>    x<sub>n</sub> = &sum; &alpha;<sub>i</sub>x<sub>i</sub></code></pre><p>for some scalar values {&alpha;x<sub>1</sub>&alpha;x<sub>2</sub>. . . &alpha;x<sub>n-1</sub>}</p><p>The column rank of a matrix is the largest subset of column vectors that are linearly independent. Similarly, the row rank is the largest subset of row vectors that are linearly independent.</p><p>The rank can be roughly considered as the information contained in the matrix. It can be proved that for any matrix A, the column rank is always equal to the row rank. Hence it is generally referred as the rank of the matrix. Some interesting properties of matrix ranks:</p><ul><li>For a matrix A &isin; R<sup>m x n</sup>, the rank(A) = &lt;= min(m,n). If rank(A) = min(m,n), it is called a full rank matrix.</li><li>For any matrix A, rank(A) = rank(A<sup>T</sup>)</li><li>For any matrix A &isin; R<sup>m x n</sup> and B &isin; R<sup>n x p</sup>, the rank(AB) &lt;= min(rank(A), rank(B))</li><li>For any matrix A,B &isin; R<sup>m x n</sup>, rank(A + B) &lt;= rank(A) + rank(B)</li></ul><h3>Inverse</h3><hr/><p>Concept of matrix inverse is very important for most linear algebra problems. Several mathematical libraries are dedicated to just calculating the inverse in an efficient way.</p><p>The inverse of a matrix A &isin; R<sup>n x n</sup> is another matrix A<sup>-1</sup> &isin; R<sup>n x n</sup> such that</p><pre><code class='python'>    A<sup>-1</sup>A = AA<sup>-1</sup> = I<sup>n</sup></code></pre><p>The matrix inverse is unique. A matrix cannot have multiple inverses. Also, the inverse may not be defined for every matrix. It is certainly not defined for non-square matrices. Even with square matrices, the inverse may not be defined. Such a matrix is called Singular Matrix.</p><p>A matrix is called non-singular or invertible if and only if A<sup>-1</sup> exists. Else it is singular or non-invertible. A non-singular or invertible matrix is always a full rank matrix.</p><p>For matrices A, B &isin; R<sup>n x n</sup>, we have</p><ul><li>(A<sup>-1</sup>)<sup>-1</sup> = A</li><li>(AB)<sup>-1</sup> = B<sup>-1</sup>A<sup>-1</sup></li><li>(A<sup>-1</sup>)<sup>T</sup> = (A<sup>T</sup>)<sup>-1</sup>. Hence it is also referred as A<sup>-T</sup></li></ul><p>For the linear equations we saw above,</p><pre><code class='python'>       Ax = b
    =&gt; x = A<sup>-1</sup>b</code></pre><p>That makes the job pretty simple. Of course, for this we require that A is a full rank square matrix. That is, we have as many equations as the number of variables and none of the equations is redundant. These are the necessary and sufficient conditions for a set of linear equations to be solvable.</p><h3>Orthogonal &amp; Orthonormal</h3><hr/><p>For two vectors x, y &isin; R<sup>n</sup>, x<sup>T</sup>y can be considered to be the shadow of x on y (or vice-versa. The dot product is positive if x has some components in the direction of y (or y in the direction of x) and it is negative if x has some components in direction of y (or y in the direction of x). In a sense, it is an indicator of the angle between them. The two vectors x, y &isin; R<sup>n</sup> are orthogonal if this dot product is 0. </p><p>And a vector x &isin; R<sup>n</sup> is called normalized if its L<sub>2</sub> norm is 1. Two vectors are called orthonormal if both are normalized and orthogonal.</p><p>When we talk about matrices, the definition of orthogonality is slightly different - although intuitively, it means the same. A square matrix A &isin; R<sup>n x n</sup> is orthogonal if all its columns are normalized and orthogonal to each other. From this definition, it follows that</p><pre><code class='python'>    UU<sup>T</sup> = I = U<sup>T</sup>U</code></pre><p>In other words, the transpose of an orthogonal matrix is also its inverse.</p><p>Note that we need a square matrix. If A is not a square matrix with orthonormal columns, AA<sup>T</sup> will be I. But since we have more rows than the columns - more rows than the dimensions, they can never by orthonormal. (eg: three vectors in 2D space can never be orthogonal to each other). Thus the A<sup>T</sup>A is not I.</p><p>Thus, we need a square matrix for it to be orthogonal.</p><p>The orthogonal matrices have some interesting properties. For example, when we multiple a vector with an orthogonal matrix, its Euclidean (L<sub>2</sub>) norm remains unchanged. Thus, for an orthogonal matrix U &isin; R<sup>n x n</sup></p><pre><code class='python'>    ||Ux||<sub>2</sub> = ||x||<sub>2</sub></code></pre><p>For any x &isin; R<sup>n</sup></p>
<h3>Range &amp; Null Space</h3><hr/><p>The span of something refers to the extent of its capacity. The span of a set of vectors {x<sub>1</sub>, . . . x<sub>n</sub>} is the set of all the vectors that can be referred by a linear combination of these. For example, the vectors (0, 1) and (1, 0) can span the entire two dimensional space. But the vectors (1, 1) and (2, 2) can span only one line in the two dimensional space. Thus the span of the set {(1, 1) and (2, 2)} is just a line while the span of the set {(0, 1), (1, 0)} is a plane.</p><p>In formal terms,</p><pre><code class='python'>    span({x<sub>1</sub>, . . . x<sub>n</sub>}) = {v : v = &sum; &alpha;<sub>i</sub>x<sub>i</sub>, &alpha;<sub>i</sub> &isin; R<sup>n</sup>}</code></pre><p>If we have n vectors are linearly independent, their span is R<sup>n</sup>.</p><p>The projection of a vector y &isin; R<sup>m</sup>} onto the span of n linearly independent vectors {x<sub>1</sub>, . . . x<sub>n</sub>} &isin; R<sup>m</sup>} (here, m &gt; n), is the vector v &isin; span{x<sub>1</sub>, . . . x<sub>n</sub>} such that y is as close to v as possible.</p><p>Formally:</p><pre><code class='python'>    proj(y; {x<sub>1</sub>, . . . x<sub>n</sub>}) = argmin<sub> v &isin; span({x<sub>1</sub>, . . . x<sub>n</sub>})</sub>(||y - v||<sub>2</sub>) </code></pre><p>The Range R(A) also called the columnspace of a matrix is essentially the span of its columns. For a matrix A &isin; R<sup>m x n</sup>, the range is defined as</p><pre><code class='python'>    R(A) = {v &isin; R<sup>m</sup> : v = Ax , x &isin; R<sup>n</sup>}</code></pre><p>This gives us some interesting properties.</p><p>If A is full rank and m &gt; n, the projection works out to be</p><pre><code class='python'>    proj(y, A) = A(A<sup>T</sup>A)<sup>-1</sup>A<sup>T</sup>y</code></pre><p>Finally, the null space of a vector A &isin; R<sup>m x n</sup>, denoted by N(A), is the set of all the vectors that equal 0 when multiplied by A. That is:</p><pre><code class='python'>    N(A) = {x &isin; R<sup>n</sup>: Ax = 0}</code></pre><p>Interestingly, R(A<sup>T</sup>) and N(A) are disjoint sets and span the entire n dimensional space. Thus, any point in R<sup>n</sup> is either in R(A<sup>T</sup>) or N(A); and never in both.</p><p>Hence they are called orthogonal complements of each other.</p>
<h3>Determinant</h3><hr/><p>The determinant of a matrix |A| or det(A) can be considered as the "volume" of space covered by its rows - the volume of the hyper cube defined by points that are a linear combination of the row vectors (a<sub>i</sub>), such that the linear coefficients are less than or equal to 1. That is, the volume covered by the set S such that</p><pre><code class='python'>    S = {v &isin; R<sup>n</sup> : v = &sum; &alpha;<sub>i</sub>a<sub>i</sub> where 0 &lt;= &alpha;<sub>i</sub> &lt;= 1}</code></pre><p>The absolute value of the determinant is the volume covered by this set S.</p><p>Algebraically; for A &isin; R<sup>n x n</sup> consider A<sub>\i\j</sub> &isin; R<sup>(n-1) x (n-1)</sup> be the matrix generated by discarding the i<sup>th</sup> row and j<sup>th</sup> column. Then, the general recursive formula for the determinant is:</p><pre><code class='python'>    |A| = &sum;<sub>i</sub> (-1)<sup>(i+j)</sup> a<sub>ij</sub>|A<sub>\i\j</sub>| for any j in 1 .. n
        = &sum;<sub>j</sub> (-1)<sup>(i+j)</sup> a<sub>ij</sub>|A<sub>\i\j</sub>| for any i in 1 .. n</code></pre><p>The determinant is ugly to calculate. But people have developed efficient libraries that help us with this. It is important to understand the concept, and leave the algebra to the machines.</p><p>Intuitively, we can think of the determinant as the difference between the two sets of extended diagonals products. For a  3x3 matrix, the determinant can be calculated as</p>
<pre><code class='python'>    |A| = a<sub>11</sub>a<sub>22</sub>a<sub>33</sub> + a<sub>12</sub>a<sub>23</sub>a<sub>31</sub> + a<sub>13</sub>a<sub>21</sub>a<sub>32</sub> - a<sub>11</sub>a<sub>23</sub>a<sub>32</sub>  - a<sub>12</sub>a<sub>21</sub>a<sub>33</sub> - a<sub>13</sub>a<sub>23</sub>a<sub>31</sub></code></pre><p>The same is extended for higher order matrices.</p>
<h3>Quadratic Forms</h3><hr/><p>Given a square matrix A &isin; R<sup>n x n</sup> and a vector x &isin; R<sup>n</sup>, the scalar value obtained by x<sup>T</sup>Ax is defined as the quadratic form. Note that:</p><pre><code class='python'>    x<sup>T</sup>Ax = &sum; x<sub>i</sub>(Ax)<sub>i</sub> = &sum; x<sub>i</sub> &sum; A<sub>ij</sub>x<sub>j</sub> = &sum;<sub>i</sub> &sum;<sub>j</sub> A<sub>ij</sub>x<sub>i</sub>x<sub>j</sub></code></pre><p>Also, since the value is a scalar,</p><pre><code class='python'>    x<sup>T</sup>Ax = (x<sup>T</sup>Ax)<sup>T</sup> = x<sup>T</sup>A<sup>T</sup>x = x<sup>T</sup>(A + A<sup>T</sup>)x/2</code></pre><p>From this, we can conclude that only the symmetric part of A contributes to the quadratic form. Hence, we implicitly assume that matrices appearing in the quadratic form are symmetric.</p><p>Based on this, for a non zero vector x &isin; R<sup>n</sup>, a symmetric matrix A &isin; S<sup>n</sup> is</p><ul><li>Positive Definite (PD) - If x<sup>T</sup>Ax &gt; 0 - Denoted by A &gt; 0</li><li>Positive Semi-Definite (PSD) - If x<sup>T</sup>Ax &gt;= 0 - Denoted by A &gt;= 0</li><li>Negative Definite (ND) - If x<sup>T</sup>Ax &lt; 0 - Denoted by A &lt; 0</li><li>Negative Semi-Definite (NSD) - If x<sup>T</sup>Ax &lt;= 0 - Denoted by A &gt; 0</li><li>Indefinite if none of the above - There exist x<sub>1</sub> and x<sub>2</sub> such that x<sub>1</sub><sup>T</sup>Ax<sub>1</sub> &gt; 0 and x<sub>2</sub><sup>T</sup>Ax<sub>2</sub> &lt; 0.</li></ul><p>Obviously, if A is positive definite, -A is negative definite; and so on. An important property of positive definite and negative definite matrices is that they are always full rank and invertible.</p><p>Gram Matrix presents an interesting case. It is defined as a matrix  that can be expressed as G = A<sup>T</sup>A - for any A &isin; R<sup>m x n</sup>. Note that A could be any matrix, need not be square. Any Gram Matrix G is always Positive Semi-Definite. Further, if m &gt;= n and A is full rank, G = A<sup>T</sup>A  is Positive Definite.</p>
<h3>Eigen Vectors &amp; Eigen Values</h3><hr/><p>Given a square matrix A &isin; R<sup>n x n</sup>, we say that &lambda; &isin; C is the eigenvalue and x &isin; R<sup>n</sup> is the corresponding eigenvector if Ax = &lambda;x for x &ne; 0. Note that &lambda; and x need not be real. They could be complex.</p><p>Intuitively, if we multiply a matrix by the eigenvector, the result points in the same direction as x, but scaled by a factor &lambda;. </p><p>Note that for any scalar c, if x is an eigenvector, cx is also an eigenvector. Thus, when we refer to the eigenvector for the eigenvalue &lambda, we imply the one that is normalized to 1. This too involves the confusion between x and -x. But that is not a major problem.</p><p>We can rewrite the above equation as</p><pre><code class='python'>    (&lambda;I - A)x = 0</code></pre><p>Interestingly, this means that (&lambda;I - A) has a non empty null space - implying that it is a singular matrix - implying that its determinant is 0. Thus, we have </p><pre><code class='python'>    |(&lambda;I - A)| = 0.</code></pre><p>We can use the definition of determinant to solve this n<sup>th</sup> order polynomial equation and get n complex values of &lambda; and then we can solve further to find the individual eigenvectors.</p><p>Of course solving an n<sup>th</sup> order polynomial is not a joke - for high values of n. There are better ways of solving this problem. But  this was the simplest way that can sufficiently elaborate the concept. Some interesting properties for A &isin; R<sup>n x n</sup></p><ul><li>The trace of A is equal to the sum of its eigenvalues</li><li>The determinant of A is equal to the product of its eigenvalues.</li><li>The rank of A is equal to the number of non-zero eigenvalues.</li><li>If A is invertible, (1/&lambda;<sub>i</sub>) is the eigenvalue of A<sup>-1</sup></li><li>The eigenvalues of a diagonal matrix are the individual diagonal elements.</li></ul><p>We can write the entire eigenvector equation as</p><pre><code class='python'>    AX = X&Lambda;</code></pre><p>Where columns of X are the eigenvectors and &Lambda; is the diagonal matrix with the diagonal elements are the corresponding eigenvalues. If the eigenvectors of A are linearly independent then X is invertible. In that case, we can rewrite the above equation as</p><pre><code class='python'>    A = X&Lambda;X<sup>-1</sup></code></pre><p>A matrix  that can be written in this form is called diagonalizable.</p>

</body><script>loadPageFormat();</script></html>
