<!DOCTYPE html>
<html>

<head>
    <title>An example of Regression using Sklearn</title>
    <meta name="description" content="Implement a A sample Regression with Sklearn">
    <meta name="keywords" content="AI, Machine Learning, Python, Sklearn">

    <meta http-equiv="x-ua-compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="author" content="Vikas K. Solegaonkar">
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="vendor/highlight/styles/default.css">
    <link href="css/blog-post.css" rel="stylesheet">
</head>

<body>
    <div id="body">

        <h1>An example of Regression using Sklearn</h1>
        <hr />
        <p>SciktLearn provides easy implementation for most regression algorithms. We can check out the major ones here. Just as all machine learning code, these modules are concept heavy and code
            lite. The code is trivially small. Just a couple of lines will do the job. But is very important to understand what happens underneath so that we can use these modules effectively.</p>
        <h2>Insurance Dataset</h2>
        <hr />
        <p>One of the early adopters of the analytics and machine learning techniques was the insurance industry. In order to price their products, the medical insurance companies need an estimate of
            the medical cost an individual could incur. This data is very easy to collect and it is continuously enriched with newer and newer observations. Hence it is a useful set for learning the
            techniques of regression.</p>
        <p>This data set contains a good amount of details about various parameters that could have an impact on the health and medical expenses of an individual. Along with this, it contains the
            actual expenses incurred. Based on this data, we can get a reasonable prediction of how a new candidate would perform based on the available information about these parameters.</p>
        <p>Let us now try to analyze the data to understand more.</p>
        <h2>Analyze the Data</h2>
        <hr />
        <p>Before jumping into the implementation of the machine learning model, it is always a good practice to peep into the data (at least a small sample), to understand how it looks. This can
            provide a great help in deciding which algorithm could be more suitable. So, let us work on that.</p>
        <p>Let us start with importing the necessary Python modules</p>
        <pre><code class='python'>import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt</code></pre>
        <p>We need to configure the Seaborn and MatplotLib - so that we can see the output on the JupyterNotebook that we use for the work. We can also set the Seaborn pallet to get the colors of our
            choice</p>
        <pre><code class='python'>sns.set_palette('husl')
%matplotlib inline</code></pre>
        <p>With this in place, let us load the Insurance data into a Pandas Data Frame. We need to give the appropriate path to the CSV file so that it can be loaded correctly.</p>
        <pre><code class='python'>data = pd.read_csv('Insurance.csv')</code></pre>
        <p>The next step is to look at the data. The dataset is pretty big and we cannot possibly look at each record there. But peeping into the first few records is quite helpful.</p>
        <pre><code class='python'>data.head()</code></pre>
        <table>
            <tr>
                <th>&nbsp;</th>
                <th>age </th>
                <th>sex </th>
                <th>bmi </th>
                <th>children </th>
                <th>smoker </th>
                <th>region </th>
                <th>charges</th>
            </tr>
            <tr>
                <td>0 </td>
                <td>19 </td>
                <td>female </td>
                <td>27.900 </td>
                <td>0 </td>
                <td>yes </td>
                <td>southwest </td>
                <td>16884.92400</td>
            </tr>
            <tr>
                <td>1 </td>
                <td>18 </td>
                <td>male </td>
                <td>33.770 </td>
                <td>1 </td>
                <td>no </td>
                <td>southeast </td>
                <td>1725.55230</td>
            </tr>
            <tr>
                <td>2 </td>
                <td>28 </td>
                <td>male </td>
                <td>33.000 </td>
                <td>3 </td>
                <td>no </td>
                <td>southeast </td>
                <td>4449.46200</td>
            </tr>
            <tr>
                <td>3 </td>
                <td>33 </td>
                <td>male </td>
                <td>22.705 </td>
                <td>0 </td>
                <td>no </td>
                <td>northwest </td>
                <td>21984.47061</td>
            </tr>
            <tr>
                <td>4 </td>
                <td>32 </td>
                <td>male </td>
                <td>28.880 </td>
                <td>0 </td>
                <td>no </td>
                <td>northwest </td>
                <td>3866.85520</td>
            </tr>
        </table>
        <p>This gives us an idea about what we have in the data frame. The insurance data has several records containing these aspects of different individuals, along with the charges they incurred.
            Now it is easier to understand the problem at hand. </p>
        <p>Note that it is not really essential to understand all these aspects about the data. A good algorithm should be able to generate a good prediction model irrespective of the actual meaning
            of each of these input features - that is the whole point behind using machine learning.</p>
        <p>But it always helps to use what we already know, so that we can get more accuracy with what we do not know.</p>
        <p>Next, we can check out the data in a wider scope. What are the data types?, Do we have any missing values?, etc. With this information, we can take some implementation level decisions.</p>
        <pre><code class='python'>data.info()</code></pre>
        <p>This generates output:</p>
        <pre><code class='python'>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 1338 entries, 0 to 1337
Data columns (total 7 columns):
age         1338 non-null int64
sex         1338 non-null object
bmi         1338 non-null float64
children    1338 non-null int64
smoker      1338 non-null object
region      1338 non-null object
charges     1338 non-null float64
dtypes: float64(2), int64(2), object(3)
memory usage: 73.2+ KB</code></pre>
        <p>This helps us with implementation specific details. We have 1338 records. Moreover, we are lucky that all the values are present. We often find ourselves in a situation where some records
            have missing fields. We need to fix that before we proceed.</p>
        <p>But, one bad news is that we have some text fields. Regression requires numbers. We need to convert the available data into numbers. But we have a catch here. The text data we have is not
            ordinal. There is no particular sorting order in there. For a linear regression to work well, we need sorted data - the output should consistently increase or decrease with increase in the
            value.</p>
        <p>We don't have a problem with the binary data. Because they are sorted anyway. But we have a serious problem with the regions. A simple solution to this problem is to split it into a set of
            binary fields. For example, we can split the four directions into two fields of two values each (north/south and east/west). This may not always work - specially when we have several
            possible values. In such a case, we forego the information in that field or accept that as a limit of the linear regression model and try something better.</p>
        <p>Let's work on that</p>
        <pre><code class='python'>data["north_south"] = np.where(((data["region"] == "northeast") | (data["region"] == "northwest")), 1, 0)
data["east_west"] = np.where(((data["region"] == "northeast") | (data["region"] == "southeast")), 1, 0)
data["sex"] = np.where(data["sex"] == "female", 1, 0)
data["smoker"] = np.where(data["smoker"] == "yes", 1, 0)
data = data.drop(["region"], axis=1)
data.head()</code></pre>
        <p>Here, we have split the region field into two fields that represent the north/south and the field that represents east/west. We have also changed all these binary fields into numeric values
            represented by 0 and 1. The actual numbers do not really matter - so long as they are different. Commonly developers use -1, 1 or 0, 1.</p>
        <p>Now, this is what we have:</p>
        <table>
            <tr>
                <th>&nbsp; </th>
                <th>age </th>
                <th>sex </th>
                <th>bmi </th>
                <th>children </th>
                <th>smoker </th>
                <th>charges </th>
                <th>north_south </th>
                <th>east_west</th>
            </tr>
            <tr>
                <td>0 </td>
                <td>19 </td>
                <td>1 </td>
                <td>27.900 </td>
                <td>0 </td>
                <td>1 </td>
                <td>16884.92400 </td>
                <td>0 </td>
                <td>0</td>
            </tr>
            <tr>
                <td>1 </td>
                <td>18 </td>
                <td>0 </td>
                <td>33.770 </td>
                <td>1 </td>
                <td>0 </td>
                <td>1725.55230 </td>
                <td>0 </td>
                <td>1</td>
            </tr>
            <tr>
                <td>2 </td>
                <td>28 </td>
                <td>0 </td>
                <td>33.000 </td>
                <td>3 </td>
                <td>0 </td>
                <td>4449.46200 </td>
                <td>0 </td>
                <td>1</td>
            </tr>
            <tr>
                <td>3 </td>
                <td>33 </td>
                <td>0 </td>
                <td>22.705 </td>
                <td>0 </td>
                <td>0 </td>
                <td>21984.47061 </td>
                <td>1 </td>
                <td>0</td>
            </tr>
            <tr>
                <td>4 </td>
                <td>32 </td>
                <td>0 </td>
                <td>28.880 </td>
                <td>0 </td>
                <td>0 </td>
                <td>3866.85520 </td>
                <td>1 </td>
                <td>0</td>
            </tr>
        </table>
        <p>This looks a lot more manageable. Next, we can look up some statistical information about the available data. What is the kind of values we have? What is the mean, variance, min / max
            values, percentiles, quartiles... These give us some more ideas about how the model should look.</p>
        <pre><code class='python'>data.describe()</code></pre>
        <p>It generates this output:</p>
        <table>
            <tr>
                <th>&nbsp; </th>
                <th>age </th>
                <th>sex </th>
                <th>bmi </th>
                <th>children </th>
                <th>smoker </th>
                <th>charges </th>
                <th>north_south </th>
                <th>east_west</th>
            </tr>
            <tr>
                <td>count </td>
                <td>1338.000000 </td>
                <td>1338.000000 </td>
                <td>1338.000000 </td>
                <td>1338.000000 </td>
                <td>1338.000000 </td>
                <td>1338.000000 </td>
                <td>1338.000000 </td>
                <td>1338.000000</td>
            </tr>
            <tr>
                <td>mean </td>
                <td>39.207025 </td>
                <td>0.494768 </td>
                <td>30.663397 </td>
                <td>1.094918 </td>
                <td>0.204783 </td>
                <td>13270.422265 </td>
                <td>0.485052 </td>
                <td>0.514200</td>
            </tr>
            <tr>
                <td>std </td>
                <td>14.049960 </td>
                <td>0.500160 </td>
                <td>6.098187 </td>
                <td>1.205493 </td>
                <td>0.403694 </td>
                <td>12110.011237 </td>
                <td>0.499963 </td>
                <td>0.499985</td>
            </tr>
            <tr>
                <td>min </td>
                <td>18.000000 </td>
                <td>0.000000 </td>
                <td>15.960000 </td>
                <td>0.000000 </td>
                <td>0.000000 </td>
                <td>1121.873900 </td>
                <td>0.000000 </td>
                <td>0.000000</td>
            </tr>
            <tr>
                <td>25% </td>
                <td>27.000000 </td>
                <td>0.000000 </td>
                <td>26.296250 </td>
                <td>0.000000 </td>
                <td>0.000000 </td>
                <td>4740.287150 </td>
                <td>0.000000 </td>
                <td>0.000000</td>
            </tr>
            <tr>
                <td>50% </td>
                <td>39.000000 </td>
                <td>0.000000 </td>
                <td>30.400000 </td>
                <td>1.000000 </td>
                <td>0.000000 </td>
                <td>9382.033000 </td>
                <td>0.000000 </td>
                <td>1.000000</td>
            </tr>
            <tr>
                <td>75% </td>
                <td>51.000000 </td>
                <td>1.000000 </td>
                <td>34.693750 </td>
                <td>2.000000 </td>
                <td>0.000000 </td>
                <td>16639.912515 </td>
                <td>1.000000 </td>
                <td>1.000000</td>
            </tr>
            <tr>
                <td>max </td>
                <td>64.000000 </td>
                <td>1.000000 </td>
                <td>53.130000 </td>
                <td>5.000000 </td>
                <td>1.000000 </td>
                <td>63770.428010 </td>
                <td>1.000000 </td>
                <td>1.000000</td>
            </tr>
        </table>
        <h2>Plotting the Data</h2>
        <hr />
        <p>With this in place, let us try to look for more. Numbers don't speak enough. To get a better feel of the data, and dig deeper, we can use plots. These are visual representations of the
            available data. These help us develop a mental picture of the data. In a real case study, we would work on a small sample set of the available data. But in this case, we can use the entire
            data set. Let us start with age</p>
        <pre><code class='python'>plt.scatter(data["age"], data["charges"], alpha=0.5)</code></pre>
        <div class='center'><img class="image" src='img/insurance_scatter_age.png' /></div>
        <p>There is some correlation between the age and the expenditure. Similarly, we can try for all other fields.</p>
        <h4>Sex</h4>
        <pre><code class='python'>plt.scatter(data["sex"], data["charges"], alpha=0.5)</code></pre>
        <div class='center'><img class="image" src='img/insurance_scatter_sex.png' /></div>
        <h4>BMI</h4>
        <pre><code class='python'>plt.scatter(data["bmi"], data["charges"], alpha=0.5)</code></pre>
        <div class='center'><img class="image" src='img/insurance_scatter_bmi.png' /></div>
        <h4>Children</h4>
        <pre><code class='python'>plt.scatter(data["children"], data["charges"], alpha=0.5)</code></pre>
        <div class='center'><img class="image" src='img/insurance_scatter_children.png' /></div>
        <h4>Smoker</h4>
        <pre><code class='python'>plt.scatter(data["smoker"], data["charges"], alpha=0.5)</code></pre>
        <div class='center'><img class="image" src='img/insurance_scatter_smoker.png' /></div>
        <h4>North-South</h4>
        <pre><code class='python'>plt.scatter(data["north_south"], data["charges"], alpha=0.5)</code></pre>
        <div class='center'><img class="image" src='img/insurance_scatter_northsouth.png' /></div>
        <h4>East-West</h4>
        <pre><code class='python'>plt.scatter(data["east_west"], data["charges"], alpha=0.5)</code></pre>
        <div class='center'><img class="image" src='img/insurance_scatter_eastwest.png' /></div>
        <p>We can note in each of these plots that there is some correlation between the field and the charges. Thus, each field is important. But some are less important than the others. The region
            and sex have only a minor impact.</p>
        <h2>Regression</h2>
        <hr />
        <p>Let us now try to take the mathematical route. In this case, we can check up many different algorithms and check out how they look. SktLearn allows us to configure hyperparameters for each
            of these algorithms - which can greatly impact the performance. But for now, we will just use the defaults and see how this works.</p>
        <p>First, we need to rearrange and split the data to get things setup. Let us start with importing the necessary modules</p>
        <pre><code class='python'>from sklearn import metrics
from sklearn.model_selection import train_test_split</code></pre>
        <p>Now we should separate the input and output values. Charges define the output while the others are input features.</p>
        <pre><code class='python'>X = data.drop(['charges'], axis=1)
y = data['charges']</code></pre>
        <p>Next, we need to split the data into the train and test sets.</p>
        <pre><code class='python'>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=5)</code></pre>
        <p>Let us now start with individual algorithms</p>
        <h4>Linear Regression</h4>
        <pre><code class='python'>from sklearn import linear_model
reg = linear_model.LinearRegression()
reg.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(reg.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(reg.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.757 and test set accuracy as 0.737</p>
        <h4>Ridge Regression</h4>
        <pre><code class='python'>from sklearn import linear_model
reg = linear_model.Ridge (alpha = .5)
reg.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(reg.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(reg.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.757 and test set accuracy as 0.737</p>
        <h4>Lasso Regression</h4>
        <pre><code class='python'>from sklearn import linear_model
reg = linear_model.Lasso(alpha = .5)
reg.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(reg.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(reg.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.757 and test set accuracy as 0.737</p>
        <h4>Bayesian Regression</h4>
        <pre><code class='python'>from sklearn import linear_model
reg = linear_model.BayesianRidge(compute_score=True)
reg.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(reg.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(reg.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.757 and test set accuracy as 0.737</p>
        <h3>Higher Order</h3>
        <hr />
        <p>Note that the accuracy is quite similar in all the above models. That is because they are all linear models and can not do much when the relation is far from linear. Let us now try some
            higher orders</p>
        <h4>Polynomial Regression</h4>
        <pre><code class='python'>from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
reg = Pipeline([('poly', PolynomialFeatures(degree=2)),
                    ('linear', LinearRegression(fit_intercept=False))])
reg.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(reg.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(reg.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.851 and test set accuracy as 0.833. That is a good jump. But again, there is a limit. We can try playing around with the degree - to see how soon it
            overfits the data.</p>
        <h4>Neural Network</h4>
        <pre><code class='python'>from sklearn.neural_network import MLPRegressor
reg = MLPRegressor(hidden_layer_sizes = (10,6), activation='relu', solver='lbfgs')
reg.fit(X_train, y_train)
print('Accuracy on the training subset: {:.3f}'.format(reg.score(X_train, y_train)))
print('Accuracy on the test subset: {:.3f}'.format(reg.score(X_test, y_test)))</code></pre>
        <p>This reports the training set accuracy 0.839 and test set accuracy as 0.831. We can further play around with the network structure and different hyper-parameters to improve this further.
        </p>
        <p>If we want to improve this further, we can also try to level the input parameters. We will discuss that in the following blogs.</p>

    </div>
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script src="vendor/highlight/highlight.pack.js"></script>
    <script src="scripts/blog.js"></script>
</body>

</html>